---
title: Manual instrumentation
description:: Manual code instrumentation and best practices for tracing with Laminar
---

## Use `observe` to group separate LLM calls in one trace

Automatic instrumentation creates spans for LLM calls within the current trace context.
That is, by default, each LLM call will create a new trace.

If you want to group several auto-instrumented calls in one trace,
simply `observe` the top-level function that makes these calls.

### Example

In this example, the `request_handler` makes a call to OpenAI to determine the user
intent. If the intent matches the expected value, it makes another call to
OpenAI (possibly with additional RAG) to generate a response.

<Tabs>
<Tab title="Python">

`request_handler` is observed, so all calls to OpenAI inside it are grouped in one trace.

```python
from lmnr import Laminar, observe
from openai import OpenAI
import os

Laminar.initialize(project_api_key=os.environ['LMNR_PROJECT_API_KEY'])

openai_client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])

@observe()
def request_handler(user_message: str):
    router_prompt = """Your goal is to determine if the user is asking for \
help with onboarding to the service. Answer 'yes' or 'no' without any \
explanation.""" + f"User message: {user_message}"

    user_intent = openai_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {
                "role": "user",
                "content": router_prompt
            }
        ]
    )
    if user_intent.choices[0].message.content == "yes":
        # likely some RAG here to enrich the context
        # ...

        model_response = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "user",
                    "content": user_message
                }
            ]
        )
        return model_response.choices[0].message.content
    return "the user is not asking for help with onboarding to the service"
```
</Tab>
<Tab title="JavaScript/TypeScript">

`requestHandler` is observed, so all calls to OpenAI are grouped in one trace.

```javascript
import { Laminar, observe } from '@lmnr-ai/lmnr';
import { OpenAI } from 'openai';

Laminar.initialize({ projectApiKey: process.env.LMNR_PROJECT_API_KEY });

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

const routerPrompt = "Your goal is to determine if the user is asking for " +
  "help with onboarding to the service. Answer 'yes' or 'no' without any " +
  "explanation. User message: ";

const handle = async (userMessage: string) => 
    await observe({name: 'requestHandler'}, async () => {
        const routerResponse = await openai.chat.completions.create({
            messages: [
                {role: 'user', content: routerPrompt + userMessage}
            ],
            model: 'gpt-4o-mini',
        })
        
        const userIntent = routerResponse
            .choices[0]
            .message;

        if (userIntent === 'yes') {
            // Likely some RAG here to enrich the context
            // ...
            const modelResponse = await openai.chat.completions.create({
                messages: [
                    {role: 'user', content: userMessage}
                ],
                model: 'gpt-4o',
            });

            return modelResponse.choices[0].message.content;
        }

        return "the user is not asking for help with onboarding";
    });

/// ...

/// Finally, at exit from your program
await Laminar.shutdown();

```
</Tab>
</Tabs>

As a result, you will get a nested trace with the `request_handler` span as the top level span,
and the OpenAI calls as child spans.

![screenshot of the example span](/images/traces/nested-trace-example.png)

### `observe` in detail

This is a reference on Python `@observe` decorator and JavaScript `observe` function.

JS wrapper functions' syntax is not as clean as Python decorators, but they are functionally equivalent.
TypeScript has decorators, but they are (1) experimental, (2) only available on classes and methods, so
we've decided to provide a wrapper function syntax. This is common for OpenTelemetry, but may not be as
common for LLM observability, so we provide a reference here.

<Tabs>
<Tab title="Python">

#### Parameters

- `name` (`str|None`): name of the span. If not provided, the name of the wrapped function will be used.
For example:
```python
@observe(name="my_span")
def my_function(): # the span will be named "my_span"
    pass

@observe()
def my_function(): # the span will be named "my_function"
    pass
```

- `session_id` (`str|None`): session ID for the current trace. If you know the session ID staticallly, you can pass it here.
- `user_id` (`str|None`): user ID for the current trace. If you know the user ID staticallly, you can pass it here.

#### Inputs and outputs
- Function parameters and their values are serialized to JSON and recorded as span input.
- Function return value is serialized to JSON and recorded as span output.

For example:
```python
@observe()
def my_function(param1, param2):
    return param1 + param2

my_function(1, 2)
```

In this case, the span will have the following attributes:
- Span input (`lmnr.span.input`) will be `{"param1": 1, "param2": 2}`
- Span output (`lmnr.span.output`) will be `3`

#### Notes
- `@observe` is a decorator factory, so it must always be used with parentheses: `@observe()`.
- This decorator can be used with both synchronous and asynchronous functions.
- Streaming responses are taken care of, so if your function returns a generator, it will be observed correctly.

</Tab>
<Tab title="JavaScript/TypeScript">

#### General syntax

```javascript
await observe({ ...options }, async (param1, param2) => {
  // your code here
}, arg1, arg2);
```

#### Parameters
- `name` (`string`): name of the span. If not passed, and the function to observe is not an anonymous arrow function, the function name will be used.
- `sessionId` (`string`): session ID for the current trace. If you know the session ID staticallly, you can pass it here.
- `userId` (`string`): user ID for the current trace. If you know the user ID staticallly, you can pass it here.
- `traceType` (`'DEFAULT'|'EVENT'|'EVALUATION'`): Type of the trace. Unless it is within evaluation, it must be `'DEFAULT'`.
- `spanType` (`'DEFAULT'|'LLM'`) - Type of the span. `'DEFAULT'` is used if not specified. If the type is `'LLM'`,
   you must manually specify some attributes. This translates to `lmnr.span.type` attribute on the span.

#### Inputs and outputs
- Function parameters and their values are serialized to JSON and recorded as span input.
- Function return value is serialized to JSON and recorded as span output.

For example:

```javascript
const result = await observe({ name: 'my_span' }, async (param1, param2) => {
    return param1 + param2;
}, 1, 2);
```

In this case, the span will have the following attributes:
- Span input (`lmnr.span.input`) will be `{"param1": 1, "param2": 2}`
- Span output (`lmnr.span.output`) will be `3`

#### Notes
- `observe` is an async function, so you must `await` it.
- Streaming responses are taken care of, so if your function returns a readable stream,
it will be observed correctly.

#### Additional examples
<Expandable title="additional examples">

```javascript
const myFunction = async (param1, param2) => {
    return param1 + param2;
}
```
    
Call `myFunction` with arguments 1 and 2, and observe it. The resulting span will be named `mySpan`:
```javascript
await observe({ name: 'mySpan' }, myFunction, 1, 2);
```
    
Call myFunction with arguments 1 and 2, and observe it. The resulting span will be named `myFunction`:
```javascript
await observe({}, myFunction, 1, 2);
```

Call an anonymous function with arguments 1 and 2, and observe it. The resulting span will be called `mySpan`.
You can technically omit `name` here, but the span will not have a name, which is not recommended.
```javascript
await observe({ name: 'mySpan' }, async (param1, param2) => {
    return param1 + param2;
}, 1, 2);
```

```javascript
const myOtherFunction = async () => {
    return 42;
}
```

Calls `myOtherFunction` with no arguments, and observe it. The resulting span will be named `myOtherFunction`
```javascript
await observe({}, myOtherFunction);
```

Call an anonymous function with no arguments, and observe it. The resulting span will be named `mySpan`.
You can technically omit `name` here, but the span will not have a name,  which is not recommended.
```javascript
await observe({ name: 'mySpan' }, async () => {
    return 42;
});
```
</Expandable>
</Tab>
</Tabs>

## Trace specific parts of code

In Python, you can also use `Laminar.start_as_current_span` if you want to record a chunk of your code using `with` statement.

### Example

```python
from lmnr import Laminar
from openai import OpenAI

Laminar.initialize(project_api_key=os.environ['LMNR_PROJECT_API_KEY'])
openai_client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])

def request_handler(user_message: str):
    with Laminar.start_as_current_span(
        name="handler",
        input=user_message
    ) as span:
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "user",
                    "content": user_message
                }
            ]
        )
        result = response.choices[0].message.content
        
        Laminar.set_span_output(result) # this will set the output of the current span

        return result
```

### Laminar.start_as_current_span in detail

`Laminar.start_as_current_span` is a context manager that creates a new span and sets it as the current span.
Under the hood it uses bare OpenTelemetry `start_as_current_span` method, but it also sets some
Laminar-specific attributes.

#### Parameters
- `name` (`str`): name of the span.
- `input` (`Any`): input to the span. It will be serialized to JSON and recorded as span input.
- `span_type` (`Literal['DEFAULT'] | Literal['LLM']`): type of the span. If not specified, it will be `'DEFAULT'`.

#### Returns
- `Span`: the span object. It is a bare OpenTelemetry span, so you can use it to set attributes, etc.
We still recommend using `Laminar.set_span_output` and `Laminar.set_span_attributes` to set attributes, unless you want
to set some custom attributes.

#### Usage
`start_as_current_span` is a context manager, so it can be called with a `with` statement. It will automatically
end the span when the context is exited. It can also be used without a `with` statement, but in this case you
must manually end the span.

We recommend against the manual call, as the code may not reach the 
`span.end()` call if an exception is raised in between. In addition, accidentally mixing `with` and manual
calls may lead to accidentally ending the span twice, which will break the OpenTelemetry API.
If you absolutely have to use the manual call, we recommend at least wrapping it in a `try`/`finally` block.

#### Examples

```python
# [Recommended] `with` statement
with Laminar.start_as_current_span(name="handler", input=user_message):
    # your code here
    Laminar.set_span_attributes({"some_attribute": "some_value"})
    Laminar.set_span_output(result) # this sets the output of the current span

# [Recommended, advanced] `with` statement with custom attributes
with Laminar.start_as_current_span(name="handler", input=user_message) as span:
    # your code here
    span.set_attribute("some_attribute", "some_value")
    Laminar.set_span_output(result) # this sets the output of the current span

# [Not recommended] manual span ending
span = None
try:
    span = Laminar.start_as_current_span(name="handler", input=user_message)
    # your code here
    Laminar.set_span_output(result) # this sets the output of the current span
finally:
    if span is not None:
        span.end()
```

## Calling raw LLM APIs, reporting token usage

This is helpful if any of the following applies to you:
- You are calling an LLM API, not using their client library/SDK.
- You are using a library that is not auto-instrumented by OpenLLMetry.
- You want to report token usage for a specific API call.
- You are using an open-source/self-hosted LLM.

There are several critical attributes that need to be set on a span to ensure it appears
as an LLM span in the UI.
- `lmnr.span.type` – must be set to `'LLM'`.
- `gen_ai.response.model` – must be set to the model name returned by an LLM API (e.g. `gpt-4o-mini`).
- `gen_ai.system` – must be set to the provider name (e.g. 'openai', 'anthropic').

In addition, the following attributes can be manually addded to report token usage and costs:
- `gen_ai.usage.input_tokens` – number of tokens used in the input.
- `gen_ai.usage.output_tokens` – number of tokens used in the output.
- `llm.usage.total_tokens` – total number of tokens used in the call.
- `gen_ai.usage.input_cost`, `gen_ai.usage.output_cost`, `gen_ai.usage.cost` – can
all be reported explicitly. However, Laminar calculates the cost of the major providers
using the values of `gen_ai.usage.input_tokens`, `gen_ai.usage.output_tokens`, and `gen_ai.response.model`.

All of these values can be set in Python and JavaScript/TypeScript using static methods on `Laminar` class.

### Example

<Tabs>
<Tab title="Python">

Use `with Laminar.start_as_current_span` to create a span and set its attributes.

```python
from lmnr import Laminar, Attributes

messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "text",
                "text": "What is the longest river in the world?",
            },
        ],
    },
]
with Laminar.start_as_current_span(
    name="my_custom_llm_call",
    input=messages,
    span_type="LLM"
):
    response = requests.post(
        "https://api.custom-llm.com/v1/completions",
        json={
            "model": "custom-model-1",
            "messages": messages,
        },
    ).json()
    Laminar.set_span_output(response["choices"][0]["message"]["content"])
    Laminar.set_span_attributes(
        {
            Attributes.PROVIDER: "custom-llm.com",
            Attributes.REQUEST_MODEL: "custom-model-1",
            Attributes.RESPONSE_MODEL: response["model"],
            Attributes.INPUT_TOKEN_COUNT: response["usage"]["input_tokens"],
            Attributes.OUTPUT_TOKEN_COUNT: response["usage"]["output_tokens"],
        }
    )
```

</Tab>
<Tab title="JavaScript/TypeScript">

Use `observe` to create a span and `Laminar.setSpanAttributes` to set its attributes.

```javascript
import { Laminar, LaminarAttributes } from '@lmnr-ai/lmnr';

await observe(
    {
        name: 'requestHandler',
        spanType: 'LLM'
    }, 
    async (message: string) => {
        const response = await fetch('https://custom-llm.com/v1/completions', {
            method: 'POST',
            body: JSON.stringify({
                model: 'custom-model-1',
                messages: [
                    {
                        role: 'user',
                        content: message
                    }
                ]
            })
        }).then(res => res.json());
        Laminar.setSpanAttributes({
            [LaminarAttributes.PROVIDER]: 'custom-llm.com',
            [LaminarAttributes.REQUEST_MODEL]: 'custom-model-1',
            [LaminarAttributes.RESPONSE_MODEL]: response.model,
            [LaminarAttributes.INPUT_TOKEN_COUNT]: response.usage.input_tokens,
            [LaminarAttributes.OUTPUT_TOKEN_COUNT]: response.usage.output_tokens,
        })
    return a;
}, userMessage);
```
</Tab>
</Tabs>
