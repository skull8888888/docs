---
title: Introduction
description: OpenTelemetry-compatible tracing with Laminar
---

import LaminarInstall from '/snippets/laminar-install.mdx';
import LaminarInitialize from '/snippets/laminar-initialize.mdx';
import GetProjectApiKey from '/snippets/get-project-api-key.mdx';

## Glossary

- **Span** – a unit of work representing a single operation in your application.
Typically, a span corresponds to a function invocation or an API call.
A single "block" on the "waterfall" trace.
  - **Span attributes** – key-value pairs of data attached to a span. We use attributes to store inputs and outputs of a span
    as well as other metadata.
  - **Span path** – a special attribute used to identify where in your code a span originates from. For example, if
    a function `call_agent` calls a function `get_response`, and `get_response` then calls `openai.chat.completions.create`,
    the span path for the LLM span would be `call_agent.get_response.openai.chat`. This is helpful to group spans by their origin,
    track performance or other metrics for each particular LLM call or funciton. Span path is stored in attributes under the key
    `lmnr.span.path`.
- **Trace** – collection of spans involved in processing a request in your LLM application.
Consists of one or more nested spans. A root span (a.k.a. top span, top-level span) is the first span in a trace,
and marks the beginning and end of the trace. Trace holds spans and aggregated metadata from the spans.
- **Event** – a key-value pair of data with a timestamp representing an event within your application. Must happen within a span.
- **Session** – a collection of traces that were serving the same user or the same interaction.

## Concept

Laminar offers comprehensive tracing of your entire application.
For every run, the entire execution trace is logged, so the information you can see in the traces includes:

- Inputs and outputs of each span
- Total execution time
- Total execution tokens and cost
- Span-level execution time and token counts

## Getting started

### Installation

<LaminarInstall />

### Add 2 lines of code to instrument your code

<LaminarInitialize />

### Project API key

<GetProjectApiKey />

### Adding manual instrumentation with `observe`

<Tabs>
<Tab title="Python">

You can instrument your code by adding the `@observe()` decorator to your functions.
This is especially helpful when you want to trace specific functions, or group
separate functions into a single trace.

```python
import os
from openai import OpenAI

from lmnr import observe, Laminar as L

L.initialize(
    project_api_key=os.environ["LMNR_PROJECT_API_KEY"],
)
client = OpenAI(api_key=os.environ["OPENAI_API_KEY"])

@observe()  # annotate all functions you want to trace
def validate_topic(topic: str):
    # assume you call the database
    # or run custom logic to validate if the topic exists
    if topic == "laminar flow":
        return topic
    else:
        return "Topic not found"

# enable this at caller level, so that both `observe`d function and the 
# auto-instrumented OpenAI calls are grouped in one trace
@observe()
def poem_writer(topic="turbulence"):
    validated_topic = validate_topic(topic)

    prompt = f"write a poem about {validated_topic}"

    # OpenAI calls are automatically instrumented
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt},
        ],
    )

    poem = response.choices[0].message.content

    return poem

if __name__ == "__main__":
    print(poem_writer(topic="laminar flow"))
```

</Tab>
<Tab title="JavaScript/TypeScript">

You can instrument your functions by wraping them in an async `observe()` functions.
This is especially helpful when you want to trace specific functions, or group
separate functions into a single trace.

```javascript
import { OpenAI } from 'openai';
import { Laminar as L, observe } from '@lmnr-ai/lmnr';

L.initialize({ projectApiKey: process.env.LMNR_PROJECT_API_KEY });

const o = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

const validateTopic = async (topic: string) => {
  // assume you call the database 
  // or run custom logicto validate if the topic exists
  if (topic === "laminar flow") {
    return topic;
  } else {
    return "Topic not found";
  }
}

const poemWriter = async (topic = "turbulence") => {
  const validatedTopic = await observe( // observe the function
    { name: 'validate' },
    async () => await validateTopic(topic)
  );

  const prompt = `write a poem about ${validatedTopic}`;
  const response = await o.chat.completions.create({
    model: "gpt-4o",
    messages: [
      { role: "system", content: "You are a helpful assistant." },
      { role: "user", content: prompt }
    ]
  });

  const poem = response.choices[0].message.content;
  return poem;
}

// Observe the top level call to trace both `observe`d function and the OpenAI call
// in one trace
await observe(
  { name: 'poemWriter' },
  async () => { await poemWriter('laminar flow') }
);
```

</Tab>
</Tabs>

Learn more about instrumenting your code by checking our
[Python SDK](https://github.com/lmnr-ai/lmnr-python) and
[JavaScript/TypeScript SDK](https://github.com/lmnr-ai/lmnr-ts).


## Accessing traces

1. Go to the traces page from the navbar on the left side of the page.
1. Click on each row to see the detailed breakdown, and waterfalll of each log on the sidebar.
1. Click "Filter" and filter by the required criteria.

<Frame caption="Example traces page">
<img height="300" src="/images/traces/traces-page.png" alt="Screenshot of the traces page"/>
</Frame>

### Viewing more details

Simply click on any of the rows in the logs page and you will see the details in the side.

Click on each span to see its details, including inputs, outputs and metadata, as well as associated events.

## OpenTelemetry compatibility

Laminar's manual and automatic instrumentation is compatible with [OpenTelemetry](https://opentelemetry.io/docs/specs/otel/).
Our backend is an OpenTelemetry-compatible destination and serves ingestion through gRPC.

Instrumentations for LLM and vector DB libraries are provided by [OpenLLMetry](https://github.com/traceloop/openllmetry).

This means that you can use OpenTelemetry SDKs to send traces to Laminar, and they will be displayed in the Laminar UI.

To get started, in your application, set the OpenTelemetry exporter to the Laminar gRPC endpoint:
`https://api.lmnr.ai:8443/v1/traces`.

Read on to the [Otel section](/tracing/otel) to learn more about the OpenTelemetry
objects and attributes that Laminar uses in more detail.
