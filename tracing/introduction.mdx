---
title: Introduction
description: OpenTelemetry-compatible tracing with Laminar
---

## Glossary

- Span – a unit of work representing a single operation in your application. A single "block" on the "waterfall" trace.
- Trace – collection of spans involved in processing a request in your LLM application.
Consists of one or more nested spans. A root span is the first span in a trace, and marks the beginning and end of the trace.
Trace holds spans and aggregated metadata from the spans.
- Event – a key-value pair of data with a timestamp representing an event within your application. Must happen within a span.
- Session – a collection of traces that were serving the same user or the same interaction.

## Concept

Laminar offers comprehensive tracing and analytics of your entire application.
For every run, the entire execution trace is logged, so the information you can see in logs includes, but is not limited to:

- Total execution time
- Total execution tokens and cost
- Span-level execution time and token counts
- Inputs and outputs of each span

## Getting started

### Prerequisites

Make sure to install the package and get your API key from the Laminar dashboard.
Read more in [Installation](/overview#installation).

### Annotate your code

<Tabs>
<Tab title="Python">
By default, `Laminar.initialize()` will enable OpenLLMetry's automatic tracing of LLM calls, vector DBs calls, and more.

If you want to instrument other tools and functions, you can use `@observe()` decorator.

```python
import os
from openai import OpenAI

from lmnr import observe, Laminar as L

L.initialize(project_api_key=os.environ["LMNR_PROJECT_API_KEY"])
client = OpenAI(api_key=os.environ["OPENAI_API_KEY"])

@observe()  # annotate all functions you want to trace
def poem_writer(topic="turbulence"):
    prompt = f"write a poem about {topic}"

    # OpenAI call is automatically traced with OpenLLMetry
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt},
        ],
    )

    poem = response.choices[0].message.content

    return poem

if __name__ == "__main__":
    print(poem_writer(topic="laminar flow"))
```

</Tab>
<Tab title="JavaScript/TypeScript">

By default, `Laminar.initialize()` will enable OpenLLMetry's automatic tracing of LLM calls, vector DBs calls, and more.

If you want to instrument other tools and functions, you can wrap them in an async `observe` function.

```javascript
const { Configuration, OpenAIApi } = require("openai");
import { OpenAI } from 'openai';
import { Laminar as L, observe } from '@lmnr-ai/lmnr';

L.initialize({ projectApiKey: process.env.LMNR_PROJECT_API_KEY });

const o = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

async const poemWriter = (topic = "turbulence") => {
  const prompt = `write a poem about ${topic}`;
  const response = await client.chat.completions.create({
    model: "gpt-4o",
    messages: [
      { role: "system", content: "You are a helpful assistant." },
      { role: "user", content: prompt }
    ]
  });

  const poem = response.choices[0].message.content;
  return poem;
}

// Observe the function like this
await observe('poemWriter', async () => { await poemWriter('laminar flow') });
```

</Tab>
</Tabs>


## Accessing traces

1. Go to the traces page from the navbar on the left side of the page
1. Click on each row to see the detailed breakdown, and waterfalll of each log on the sidebar.
1. Click "Filter" and filter by the required criteria.

<Frame caption="Example traces page">
<img height="300" src="/images/traces/traces-page.png" alt="Screenshot of the traces page"/>
</Frame>

### Viewing more details

Simply click on any of the rows in the logs page and you will see the details in the side.

Click on each span to see its details, including inputs, outputs and metadata, as well as associated events.

## OpenTelemetry compatibility

Laminar is compatible with OpenTelemetry tracing through the means of [OpenLLMetry](https://github.com/traceloop/openllmetry).

This means that you can use OpenTelemetry SDKs to send traces to Laminar, and they will be displayed in the Laminar UI.

To get started, in your application,
set the OpenTelemetry exporter to the Laminar gRPC endpoint: `https://api.lmnr.ai:8443/v1/traces`.

Read on to the [Next section](/tracing/otel) to learn more about the OpenTelemetry
objects and attributes that Laminar uses.