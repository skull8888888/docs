---
title: 'Deploy on Laminar (2/2)'
description: 'Deploy your Laminar pipeline'
---

This is second tutorial (2/2) in a series about how to develop and deploy your pipeline on Laminar.

## Prerequisites

- Python - Install Python3.9+
- poetry - Install poetry for managing your dependencies in a project [(Read more)](https://python-poetry.org/docs/)
- Check out the 1st tutorial to see the sample pipeline and how to organize your code around it [(Read more)](/tutorials/first-pipeline)

## Creating endpoint

In [previous tutorial](/tutorials/first-pipeline), you built a pipeline which queries url and answers your question, and integrated your local code to the pipeline.

Now, let's deploy everything.

Let's start by creating a Laminar endpoint.
You can go to Endpoints page (by navigating in the navbar on the left) and click "New endpoint".

Let's name it `website_qa`

Endpoint will host both (1) your code and (2) your pipeline. Each function from the code will be called when the corresponding node in the pipeline is executed.

## 1. Deploy code to an endpoint

You can open terminal. Go to the root directory of your Laminar project `website_qa` (or any other project you're working on).

If you don't yet have `lmnr` package installed, run

`poetry add lmnr`

Activate your poetry shell by typing:

`poetry shell`

If you haven't set your Laminar project id, do it by typing:

`export LMNR_PROJECT_API_KEY={your project api key}`

Then run:

`lmnr deploy {endpoint_id}`

To get `endpoint_id`, go to endpoint's page and you'll see its id in UUID format.

Now you can see "code preparing" status on your endpoint page.

<Frame caption="Code preparing">
<img height="100" src="/images/tutorials/code-preparing-no-pipeline.png" alt="Code preparing status on Laminar endpoint" />
</Frame>

Wait a little bit while the container is spinning up and then you'll see "code deployed" status.

When the status changes to "code deployed", your code is deployed and your functions can be called from the pipeline. Now let's deploy the pipeline.

To explain more, behind the scenes, we manage a separate container for your code per endpoint.

It means that whenever you re-deploy your code to the endpoint, previous container will be terminated and the new one will be started.

<Tip>
    Since you have your own separate container, you can add any dependencies by running `poetry add {dependency_name}` in your directory.
</Tip>

## 2. Deploy pipeline to an endpoint

Go back to Pipelines and open your pipeline.

Click Deploy at the top of the page. Enter commit name "v1", select our endpoint "website_qa" and click "Deploy".

Your pipeline will be deployed instantly to the endpoint, which contains your code.

<Info>
    If you want to either re-deploy code or re-deploy pipeline, you can do it independently. You don't need to re-deploy both every time.
</Info>

Now everything is ready!

You can go back to your endpoint's page. Click "use API" and you'll see curl command through which you can call your endpoint.
Also, it will show the code for our Python and Typescript SDKs, which you can use in your Python and Typescript code, respectively.

Try calling:
```
curl 'https://api.lmnr.ai/v2/endpoint/run' \
-H "Content-Type: application/json" \
-H "Authorization: Bearer $LMNR_PROJECT_API_KEY" \
-d '{
  "endpoint": "website_qa",
  "inputs": {
    "question": "What is a Laminar workspace",
    "url": "https://docs.lmnr.ai/tutorials/getting-started"
  },
  "env": {
    "OPENAI_API_KEY": "$OPENAI_API_KEY"
  },
  "metadata": {}
}'
```

Note that you won't need to provide `DEBUGGER_SESSION_ID` environment variable, since that is only required when developing and debugging your code locally, before it's deployed on Laminar.

However, you'll need to pass your `OPENAI_API_KEY`.
We do provide API keys for OpenAI's gpt-3.5 models and gpt-4o-mini only while you play with pipeline in a pipeline builder, but during endpoint runs, or if you use other models (e.g. OpenAI gpt-4o or Anthropic's Claude), you'll need to bring your own API keys.

If you followed all the instructions, you must receive a response from your API call.

If you're interested in fully hosting your LLM features or agents, including your code, contact us for the beta access at [*founders@lmnr.ai*](mailto:founders@lmnr.ai).
