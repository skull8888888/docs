---
name: Overview
---

import LaminarInstall from '/snippets/laminar-install.mdx';
import LaminarInitialize from '/snippets/laminar-initialize.mdx';
import GetProjectApiKey from '/snippets/get-project-api-key.mdx';

Laminar is an open-source LLM engineering platform.

Laminar powers the data flywheel for your LLM applications. With Laminar, you can
trace, evaluate, annotate, analyze, and re-use your LLM data.

Check our [Github repo](https://github.com/lmnr-ai/lmnr) to learn more about how it works or if you are interested in self-hosting.

## Getting started

### Installation

<LaminarInstall />

### Add 2 lines to instrument your code

<LaminarInitialize />

### Project API key

<GetProjectApiKey />

## Example: Instrument OpenAI calls with Laminar

<Tabs>
<Tab title="Python">

```python
import os
from openai import OpenAI
from lmnr import Laminar as L

L.initialize(
    project_api_key=os.environ["LMNR_PROJECT_API_KEY"],
)

client = OpenAI(api_key=os.environ["OPENAI_API_KEY"])

def poem_writer(topic: str):
    prompt = f"write a poem about {topic}"

    # OpenAI calls are automatically instrumented
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt},
        ],
    )
    poem = response.choices[0].message.content
    return poem

if __name__ == "__main__":
    print(poem_writer("laminar flow"))
```

</Tab>
<Tab title="JavaScript/TypeScript">

```javascript
import { Laminar as L, observe } from '@lmnr-ai/lmnr';
L.initialize({ projectApiKey: process.env.LMNR_PROJECT_API_KEY });
/// ...
import { OpenAI } from 'openai';

const o = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

const poemWriter = async (topic = "turbulence") => {
  const prompt = `write a poem about ${topic}`;

  // OpenAI calls are automatically instrumented
  const response = await o.chat.completions.create({
    model: "gpt-4o",
    messages: [
      { role: "system", content: "You are a helpful assistant." },
      { role: "user", content: prompt }
    ]
  });

  const poem = response.choices[0].message.content;
  return poem;
}

// somewhere in async context
await poemWriter("laminar flow");
await L.shutdown();
```
</Tab>
</Tabs>

### Adding manual instrumentation

If you want to trace your own functions for their durations, inputs and outputs, or want to group 
them into one trace,
you can use `@observe` decorator in Python or async `observe` function in JavaScript/TypeScript.

<Tabs>
<Tab title="Python">

```python
import os
from openai import OpenAI
from lmnr import observe, Laminar as L

L.initialize(project_api_key=os.environ["LMNR_PROJECT_API_KEY"])
client = OpenAI(api_key=os.environ["OPENAI_API_KEY"])

@observe()
def request_handler(data: dict):
    # some other logic, e.g. data preprocessing

    response1 = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": data["prompt_1"]},
        ],
    )

    # some other logic, e.g. a conditional DB call

    response2 = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": data["prompt_2"]},
        ],
    )

    return response2.choices[0].message.content
```
</Tab>
<Tab title="JavaScript/TypeScript">

```javascript
import { OpenAI } from 'openai';
import { Laminar as L, observe } from '@lmnr-ai/lmnr';

L.initialize({ projectApiKey: process.env.LMNR_PROJECT_API_KEY });
const o = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

const requestHandler = async (data: Record<string, string>) => 
    await observe({name: 'requestHandler'}, async () => {
        // some other logic, e.g. data preprocessing

        const response1 = await o.chat.completions.create({
            model: "gpt-4o",
            messages: [
                { role: "system", content: "You are a helpful assistant." },
                { role: "user", content: data["prompt_1"] },
            ],
        });

        // some other logic, e.g. a conditional DB call

        const response2 = await o.chat.completions.create({
            model: "gpt-4o",
            messages: [
                { role: "system", content: "You are a helpful assistant." },
                { role: "user", content: data["prompt_2"] },
            ],
        });

        return response2.choices[0].message.content;
    });

```
</Tab>
</Tabs>

### Next steps

[Learn more](/tracing/configuration#use-observe-to-group-separate-llm-calls-in-one-trace) about
using `observe` and manual instrumentation.


## Features

### Observability

Track the full execution trace of your LLM application.

Laminar's instrumentation is compatible with [OpenTelemetry](https://opentelemetry.io/docs/specs/otel/).

Get started with [Tracing](/tracing/introduction).

![Screenshot of observability dashboard](/images/overview.png)

### Events and Analytics

Make sense of thousands of traces generated by your LLM application.
Evaluate and catch failures of your agents in real-time. Gather metrics on user or agent behavior based on semantic meaning.

In addition, you can track raw metrics, by sending events with values directly
to Laminar.

Learn more about [Events](/events/introduction) extraction.

### Data labeling and re-ingestion

Laminar provides you with a UI where you can label and annotate the traces of your LLM applications.
Organize the labeled data into datasets and use them to update your prompts or fine-tune your models.

Perform advanced hybrid search on datasets to augment your prompts with the most semantically relevant examples
from past traces.

Learn more about [Data labeling](/datasets/label-spans) and [Datasets](/datasets/introduction).

### Evaluations

Laminar allows you to run your prompts and models against datasets, evaluate the performance, and analyze their
results in the dashboard.

You can use Laminar's JavaScript and Python SDKs to set up and run your evaluations.

Learn more about [Evaluations](/evaluations/introduction).

### Prompt chain management

You can build and host chains of prompts and LLMs and then call them as if each chain was a single function.
It is especially useful when you want to experiment with techniques, such as Mixture of Agents
and self-reflecting agents, without or before having to manage prompts and model configs in your code.

Learn more about [Pipeline builder](/pipeline/introduction) for prompt chains.
