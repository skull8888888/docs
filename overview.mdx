---
name: Overview
---
Laminar is an open-source observability and analytics platform for complex LLM apps. 
Laminar helps developers build better LLM applications by providing a comprehensive
set of tools for observability, analytics, and prompt chain management.

Living at the intersection of tracing and event-based analytics, Laminar is
like Datadog + PostHog for LLM applications.

Check our [Github repo](https://github.com/lmnr-ai/lmnr) to learn more about how it works or if you are interested in self-hosting.

## Getting started

### Installation

<Tabs>
<Tab title="Python">

Install the package from [PyPI](https://pypi.org/project/lmnr/).

```sh
pip install lmnr
```

<Expandable title="other installation options: poetry, uv, rye">

poetry

```sh
poetry add lmnr
```

uv

```sh
uv add lmnr
```

rye 

```sh
rye add lmnr
```
</Expandable>

</Tab>
<Tab title="Javascript/Typescript">

Install the package from [npm](https://www.npmjs.com/package/@lmnr-ai/lmnr).

```sh
npm add @lmnr-ai/lmnr
```

<Expandable title="other installation options: yarn, pnpm">
yarn

```sh
yarn add @lmnr-ai/lmnr
```

pnpm

```sh
pnpm add @lmnr-ai/lmnr
```
</Expandable>
</Tab>
</Tabs>

### Code change

Major LLM providers, frameworks including LangChain and LlamaIndex, and even vector
db calls are instrumented automatically.

<Tabs>
<Tab title="Python">

```python
import os
from openai import OpenAI
from lmnr import Laminar as L

L.initialize(
    project_api_key=os.environ["LMNR_PROJECT_API_KEY"],
)

client = OpenAI(api_key=os.environ["OPENAI_API_KEY"])

def poem_writer(topic: str):
    prompt = f"write a poem about {topic}"

    # OpenAI calls are automatically instrumented
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt},
        ],
    )
    poem = response.choices[0].message.content
    return poem

if __name__ == "__main__":
    print(poem_writer("laminar flow"))
```

</Tab>
<Tab title="JavaScript/TypeScript">

```javascript
import { OpenAI } from 'openai';
import { Laminar as L, observe } from '@lmnr-ai/lmnr';

L.initialize({ projectApiKey: process.env.LMNR_PROJECT_API_KEY });

const o = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

const poemWriter = async (topic = "turbulence") => {
  const prompt = `write a poem about ${topic}`;

  // OpenAI calls are automatically instrumented
  const response = await o.chat.completions.create({
    model: "gpt-4o",
    messages: [
      { role: "system", content: "You are a helpful assistant." },
      { role: "user", content: prompt }
    ]
  });

  const poem = response.choices[0].message.content;
  return poem;
}

poemWriter("laminar flow")
    .then(console.log)
    .finally(() => L.shutdown());
```
</Tab>
</Tabs>

If you want to trace your own functions for their durations, inputs and outputs, or want to group 
them into one trace,
you can use `@observe` decorator in Python or async `observe` function in JavaScript/TypeScript.

<Tabs>
<Tab title="Python">

```python
import os
from openai import OpenAI
from lmnr import observe, Laminar as L

L.initialize(project_api_key=os.environ["LMNR_PROJECT_API_KEY"])
client = OpenAI(api_key=os.environ["OPENAI_API_KEY"])

@observe()
def request_handler(data: dict):
    # some other logic, e.g. data preprocessing

    response1 = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": data["prompt_1"]},
        ],
    )

    # some other logic, e.g. a conditional DB call

    response2 = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": data["prompt_2"]},
        ],
    )

    return response2.choices[0].message.content
```
</Tab>
<Tab title="JavaScript/TypeScript">

```javascript
import { OpenAI } from 'openai';
import { Laminar as L, observe } from '@lmnr-ai/lmnr';

L.initialize({ projectApiKey: process.env.LMNR_PROJECT_API_KEY });
const o = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

const requestHandler = async (data: Record<string, string>) => 
    await observe({name: 'requestHandler'}, async () => {
        // some other logic, e.g. data preprocessing

        const response1 = await o.chat.completions.create({
            model: "gpt-4o",
            messages: [
                { role: "system", content: "You are a helpful assistant." },
                { role: "user", content: data["prompt_1"] },
            ],
        });

        // some other logic, e.g. a conditional DB call

        const response2 = await o.chat.completions.create({
            model: "gpt-4o",
            messages: [
                { role: "system", content: "You are a helpful assistant." },
                { role: "user", content: data["prompt_2"] },
            ],
        });

        return response2.choices[0].message.content;
    });

```
</Tab>
</Tabs>

#### Next steps

[Learn more](/tracing/configuration##grouping-separate-llm-calls-in-one-trace) about using observe.

For more language-specific instructions, check our [Python SDK](https://github.com/lmnr-ai/lmnr-python)
and [JavaScript/TypeScript SDK](https://github.com/lmnr-ai/lmnr-ts).

### Project API key

To get the project API key, go to the Laminar dashboard, click the project settings,
and generate a project API key.
Unless specified at initialization, Laminar will look for the key in the `LMNR_PROJECT_API_KEY` environment variable


## Features

### Observability

With Laminar, you can instrument your entire LLM application and track the full execution trace.

Laminar's observability is based on [OpenLLMetry](https://github.com/traceloop/openllmetry),
and it is compatible with [OpenTelemetry](https://opentelemetry.io/docs/specs/otel/).

Get started with [Tracing](/tracing/introduction).

![Screenshot of observability dashboard](/images/overview.png)

### Analytics

Laminar provides infrastructure to run LLM analysis to extract semantic events,
such as "user sentiment" or "did my LLM agent upsell?", and then turn them
into trackable metrics. Combining these events with the trace data allows
you to link back to the specific user interaction that caused the event
and gives you a better understanding of the user experience.

In addition, you can track raw metrics, by sending events with values directly
to Laminar.

Learn more about [Events](/events/introduction) extraction.

### Prompt chain management

You can build and host chains of prompts and LLMs and then call them as if each chain was a single function.
It is especially useful when you want to experiment with techniques, such as Mixture of Agents
and self-reflecting agents, without or before hosting prompts and model configs in your code.

Learn more about [Pipeline builder](/pipeline/introduction) for prompt chains.

### Evaluations

In addition to semantic events, Laminar allows you to run evaluations and analyze their
results in the dashboard.

You can use Laminar's JavaScript and Python SDKs to set up and ran your evaluations.

Learn more about [Evaluations](/evaluations/introduction).
