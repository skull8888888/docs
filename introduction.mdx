---
title: Introduction
---

## Laminar

**90% of LLM workflows donâ€™t go beyond demo stage.**
Bringing them to production, requires meticulous prompt engineering, huge infra work to deploy and monitor in a scalable way.

We solve all these problems with a single platform to build, deploy, monitor, and **improve** LLM apps.
Laminar provides an extensible set of tools to help you make **reliable and cost-effective LLM apps**,
and bring it to production in seconds.

Laminar is written in Rust, which makes it blazingly fast.

## First steps

To begin with, you can [check our demo](https://www.loom.com/share/0dabced01c77478ab206766f10ab23c7) to get an overview of features available on Laminar.

After that, you can start with reading about how to [create a pipeline](/pipeline/introduction).
Try creating a simple pipeline by connecting Input node to [LLM node](/nodes/LLM), and then connecting LLM node to Output node.

Step by step, you'll unveil various functionalities of our platform and make your LLM app ready for production.

## Key features

- **Versatile pipeline builder**: Build LLM pipeline using variety of nodes with required functionality provided out of the box
- **Endpoints and checks**: Deploy your pipeline to an API endpoint in seconds, while protecting it with custom checks
- **Logging**: Conveniently access the logs and see how users interact with your pipeline through endpoint
- **Evaluations**: Evaluate your LLM pipeline with a broad set of metrics
- **Semantic Search**: Cache data or upload it so that it can be quickly accessed based on semantic similarity
- **Semantic router**: Direct the flow of pipeline by utilizing our semantic router

To see all features, you can read more in other sections.

We are adding new features every day. Feel free to contact us as *founders@lmnr.ai* for any feature requests.
