---
title: Introduction
---

## Laminar

**90% of LLM workflows donâ€™t go beyond demo stage.**
Bringing them to production, requires meticulous prompt engineering, huge infra work to deploy and monitor in a scalable way.

We solve all these problems with a single platform to build, deploy, monitor, and **improve** LLM apps.
Laminar provides an extensible set of tools to help you make **reliable and cost-effective LLM apps**,
and bring it to production in seconds.

Laminar is written in Rust, which helps it to be blazingly fast.

## First steps

To begin with, you can [check our demo](https://www.loom.com/share/e842230d351f4ca99a0715979b854ba3) to get a sense of how to use our platform.

After that, you can start with reading about how to [create a pipeline](/pipeline/introduction).
Try creating a simple pipeline by connecting Input, Output, and [Instruct](/nodes/chat-instruct-llm-nodes#instruct-node) nodes.

Step by step, you'll unveil various functionalities of our platform and make your LLM app ready for production.

## Key features

- **Flexible pipeline builder**: Build LLM pipeline using variety of nodes with required functionality provided out of the box
- **Endpoints and checks**: Deploy your pipeline to an API endpoint in seconds, while protecting it with custom checks
- **Logging**: Conveniently access the logs and see how users interact with your pipeline through endpoint
- **Evaluations**: Evaluate your LLM pipeline with a broad set of metrics
- **Semantic Search**: Cache data or upload it so that it can be quickly accessed based on semantic similarity
- **Semantic router**: Direct the flow of pipeline by utilizing our semantic router
- **Prompt copilot**: Improve your LLM prompts right on the node with convenient prompt copilot

To see all features, you can read more in other sections.

We are adding new features every day. Feel free to contact us as *founders@lmnr.ai* for any feature requests.
