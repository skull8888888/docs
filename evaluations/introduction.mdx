---
title: Introduction
description: Evaluate the results of your application
---

## Introduction

Evaluation is the concept of running your logic against some dataset
and evaluating its results with another function.


## Key concepts

- Executor – the function being evaluated, often your prompt or (to-be-)production logic
- Evaluator - the function evaluating the results
- Dataset format – two fixed JSON objects: `target` and `data` each with any keys.
  - `target` – data sent to the evaluator. Usually, contains the expected or target outputs of certain parts of your executor.
  - `data` – data sent to the executor.

## Flow overview

<Frame caption="High-level overview of the evaluation flow">
  <img
    height="300"
    src="/images/evaluations/evaluation-with-executor.png"
    alt="High-level overview of the evaluation flow with the executor pipeline"
  />
</Frame>

For every datapoint in the dataset, evaluation does the following:

1. Pass the `data` as an argument to the executor.
1. Run the executor.
1. Executor output is stored.
1. Output of the executor, and `target` are passed to the _evaluator_ function.
1. Evaluator pipeline produces a numeric output or a json object with several numeric outputs.
This is stored in the results of the evaluation.



## Quickstart

### Prerequisites

Make sure to install the package and get your API key from the Laminar dashboard.
Read more in [Installation](/overview#installation).

### Requirements and definitions

- Executor takes in a `Map<string, any>` (set to the value of `data`) parameter and returns anything
- Evaluator takes in a `Map<string, any` (set to the value of `target`) parameter and the output of the executor
- Evaluator returns either a single numeric score or a JSON object / dict, with string keys and numeric values for multiple scores

### Example. Running and registering evaluations

#### 1. Define an executor function

<Tabs>
<Tab title = "Python">
```python
from openai import AsyncOpenAI
from lmnr import Evaluation
import asyncio
import os

openai_client = AsyncOpenAI(api_key=os.environ["OPENAI_API_KEY"])

async def get_capital(data):
    country = data["country"]
    response = await openai_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {
                "role": "user",
                "content": f"What is the capital of {country}? Just name the "
                "city and nothing else",
            },
        ],
    )
    return response.choices[0].message.content.strip()

```
</Tab>
<Tab title = "JavaScript/TypeScript">
```javascript
import { Evaluation } from '@lmnr-ai/lmnr';

import OpenAI from 'openai';

const openai = new OpenAI({apiKey: process.env.OPENAI_API_KEY});

const getCapital = async ({country} : {country: string}): Promise<string> => {
    const response = await openai.chat.completions.create({
        model: 'gpt-4o-mini',
        messages: [
            {
                role: 'system',
                content: 'You are a helpful assistant.'
            }, {
                role: 'user',
                content: `What is the capital of ${country}? 
                Just name the city and nothing else`
            }
        ],
    });
    return response.choices[0].message.content ?? ''
}

```
</Tab>
</Tabs>

#### 2. Define an evaluator function

<Tabs>
<Tab title = "Python">
```python
def evaluator_A(output, target):
    return 1 if output == target["capital"] else 0
```
</Tab>
<Tab title = "JavaScript/TypeScript">

```javascript

const evaluatorA = async (output, target) =>
        (await output) === target.capital ? 1 : 0
```

</Tab>
</Tabs>

#### 3. Define data and run the evaluation

<Tabs>
<Tab title = "Python">
```python
# Evaluation data
data = [
    {"data": {"country": "Germany"}, "target": {"capital": "Berlin"}},
    {"data": {"country": "Canada"}, "target": {"capital": "Ottawa"}},
    {"data": {"country": "Tanzania"}, "target": {"capital": "Dodoma"}},
]

# Create an Evaluation instance
e = Evaluation(
    name="py-evaluation-async",
    data=data,
    executor=get_capital,
    evaluators=[evaluator_A],
    project_api_key=os.environ["LMNR_PROJECT_API_KEY"],
)

# Run the evaluation
asyncio.run(e.run())
```
</Tab>
<Tab title = "JavaScript/TypeScript">

```javascript

const evaluationData = [
    { data: { country: 'Canada' }, target: { capital: 'Ottawa' } },
    { data: { country: 'Germany' }, target: { capital: 'Berlin' } },
    { data: { country: 'Tanzania' }, target: { capital: 'Dodoma' } },
]

const e = new Evaluation( 'my-evaluation', {
    data: evaluationData,
    executor: async (data) => await getCapital(data),
    evaluators: [evaluatorA],
    config: {
        projectApiKey: process.env.LMNR_PROJECT_API_KEY
    }
})

e.run();
```

</Tab>
</Tabs>

## Status and results of the evaluation

Possible statuses of evaluations:
- Started
- Finished
- Error

Individual datapoint statuses "Success" / "Failed" are stored at datapoint level.
