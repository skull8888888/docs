---
title: Introduction
description: Evaluate the results of your LLM applications with Laminar
---

import LaminarInstall from '/snippets/laminar-install.mdx';

## Introduction

Evaluation is the concept of running your logic against some dataset
and evaluating its results with another function.

This is useful for testing your models, checking the performance of your AI agents, or comparing different versions of your propmpts.

## Key concepts

- Executor – the function being evaluated, often your prompt or (to-be-)production logic
- Evaluator - the function evaluating the results
- Dataset format – two fixed JSON objects: `target` and `data` each with any keys.
  - `target` – data sent to the evaluator. Usually, contains the expected or target outputs of certain parts of your executor.
  - `data` – data sent to the executor.

## Flow overview

<Frame caption="High-level overview of the evaluation flow">
  <img
    height="300"
    src="/images/evaluations/evaluation-with-executor.png"
    alt="High-level overview of the evaluation flow with the executor pipeline"
  />
</Frame>

For every datapoint in the dataset, evaluation does the following:

1. Pass the `data` as an argument to the executor.
1. Run the executor.
1. Executor output is stored.
1. Output of the executor, and `target` are passed to the _evaluator_ function.
1. Evaluator pipeline produces a numeric output or a json object with several numeric outputs.
This is stored in the results of the evaluation.

## Quickstart

### Prerequisites

<LaminarInstall />

### Requirements and definitions

- Executor takes in a `Map<string, any>` (set to the value of `data`) parameter and returns anything
- Evaluator takes in a `Map<string, any` (set to the value of `target`) parameter and the output of the executor
- Evaluator returns either a single numeric score or a JSON object / dict, with string keys and numeric values for multiple scores

### Example. Running and registering evaluations

#### 1. Create an evaluation file.

<Tabs>
<Tab title = "Python">

Create a file named `my-first-evaluation.py` and add the following code:

```python
# my-first-evaluation.py
from lmnr import evaluate

def write_poem(topic: str) -> str:
    # replace this with your LLM call or custom logic
    return f"This is a good poem about {topic}"

def contains_poem(output: str, target: str) -> int:
    return 1 if target in output else 0

evaluate(
    data=[
        {"data": {"topic": "flowers"}, "target": {"poem": "This is a good poem about flowers"}},
        {"data": {"topic": "cats"}, "target": {"poem": "I like cats"}},
    ],
    executor=write_poem,
    evaluators={"containsPoem": contains_poem}
)
```

</Tab>
<Tab title = "JavaScript/TypeScript">

Create a file named `my-first-evaluation.ts` and add the following code:

```javascript
// my-first-evaluation.ts
import { evaluate } from '@lmnr-ai/lmnr';

const writePoem = ({topic}: {topic: string}) => {
    // replace this with your LLM call or custom logic
    return `This is a good poem about ${topic}`
}

evaluate({
    data: [
        { data: { topic: 'flowers' }, target: { poem: 'This is a good poem about flowers' } },
        { data: { topic: 'cats' }, target: { poem: 'I like cats' } },
    ],
    executor: (data) => writePoem(data),
    evaluators: {
        containsPoem: (output, target) => target.poem.includes(output) ? 1 : 0
    }
})
```
</Tab>
</Tabs>

#### 2. Run the evaluation

You can run the evaluations both from Laminar CLI and from code.
To run the evaluation from the CLI, execute the following command:

<Tabs>
<Tab title = "Python">
```sh
# 1. Make sure `lmnr` is installed in a virtual environment
# lmnr --help
# 2. Run the evaluation
lmnr eval my-first-evaluation.py
```
</Tab>
<Tab title = "JavaScript/TypeScript">
```sh
npx lmnr eval my-first-evaluation.ts
```
</Tab>

</Tabs>

To run the evaluation directly from code, simply call the `evaluate` function, i.e. import or
run the file created in the previous step.

## Viewing evaluation results and traces

When you run an evaluation from the CLI, Laminar will output the link to the dashboard
where you can view the evaluation results.

Laminar stores every evaluation result.
A run for every datapoint is represented as a trace.
You can view the results and corresponding traces in the evaluations page.
