---
title: 'Pipeline Builder'
description: "Pipeline builder gives an ability to build LLM workflows of different complexity."
---

## Glossary

**Pipeline** – an instance of a complete workflow

**Pipeline version** – a saved snapshot of a pipeline, similar to a git commit.

**Node** – a core component of a pipeline that performs data transformation and passes the result downstream.

**Handle** – an input or output point of a node. Input handles of one node can be connected to output handles of another one.

## Connecting nodes

Every node has exactly 1 output handle on its right hand side and 0 or more input handles on its left hand side. Connect the nodes by dragging a connection from one node's output handle to another node's input handle.

TODO
image showing two nodes connected, preferrably non IO nodes

Each input handle must take exactly 1 connection, however an output handle can be connected to many other nodes. In other words, one node's output can serve as an input to multiple nodes, but not the other way around.

TODO
image showing one node serving as an input to multiple other nodes

### Handle types

Depending on the data the node expects, there are different types of handles.

- String – basic text input
- StringList – JSON array of strings, e.g. `["Hello", "World"]`
- ChatMessageList – JSON array of `ChatMessage` objects in chronological order of the chat conversation. `ChatMessage` has 2 required fields: `role` and `content`. `role` is a string, normally `user`, `assistant`, or `system`, but may vary depending on the LLM provider. `content` is string content of a message.
- \[DEPRECATED?\] ThreadChatMessage – a JSON object, same as `ChatMessage` but with an optional field `threadId`. See agent node docs(TODO) for more details.

Only handles of the same type can be connected to each other.

## Naming nodes

Every node has a name. This:
- Is required, when calling deployments, to direct inputs to the correct input nodes and correctly parse the response
- Is required for proper integration with datasets and evaluations
- Is a utility for you when analyzing logs and traces.

All nodes names within a pipeline must be unique. Pipeline execution itself can run fine with repeated node names, but other functionality may be impaired.

## Input and Output nodes

Input and Output nodes are special. They "define" the API once you deploy the pipeline. That is, your request to a Laminar endpoint API must send all inputs in a map following the pattern `"inputNodeName": "inputValue"`. Similarly, the response message contains a map from each output node's name to the output result of that node. See more examples in endpoints (TODO).

In addition, whenever you drag and drop a new input node, an input field will appear in the sidebar on the left. You must fill this field in in order to run the pipeline in Workshop mode.

### Input types

Similar to node handles, an input node has a type, which defines the type of its handle as well. For example, `StringList` input node produces a `StringList` results and can be connected only to handles expecting `StringList` value, e.g. Semantic Search's `query` handle.

For your convenience, upon new connection, an unconnected input node changes its type and name to follow the type and name of a handle it is being connected to.

## String templates

For relevant nodes, i.e. Chat and Instruct, we also allow the message to be changed with the user input between different calls. This is achieved with enclosing your `{{variables}}` in double curly braces, a.k.a moustache syntax. To start, simply enclose a word in your node's prompt/system message field, and a new handle of type `String`, and named with that enclosed word will appear on the left hand side of the node.

Sometimes, you may want to replace another input value or make longer or nested templates. For that purpose, you can use StringTemplate node and nest strings however you wish!

TODO
an image showing a slightly more sophisticated templating

## Running the pipeline

You can run the pipeline right from the builder screen to test pipeline execution. Make sure to meet all the requirements before doing so:

- Pipeline has at least one input node
- Pipeline has at least one output node
- All nodes and handles are connected
- All inputs are non-empty in the left sidebar
- If your pipeline makes LLM API calls, that all API keys are set in the API keys page

Simply hit "Run" at the top of the page, and you'll see the outputs in the bottom drawer once the execution completes.

NOTE: executions are not logged for your workshop experiments. We only logged deployed pipeline executions, i.e. when a pipeline is called via laminar API.

### Running multiple executions in parallel.

You may want to experiment with different prompts or dynamic inputs in parallel, to better asses the performance of your model at the same time. Here are some simple step to do this with laminar:

1. Build your pipeline
1. Click "Add parallel execution" in the left sidebar. Click it as many more times as you want!
1. Don't worry you can unstage an execution if you've added one by mistake
1. IMPORTANT: make sure to fill in all the inputs for all executions on the left
1. Hit "Run" and all your calls will be made in parallel.
1. Compare and study your results in the bottom drawer.
1. Repeat and have fun.

## Saving your work

You don't have to, pipelines are saved automatically! You still can click "Save" at the top of the page and all your work will be saved.

## Deploying a pipeline version

Once you are happy with your pipeline and ready to use it in external applications, be it testing or production, you can deploy your pipeline version to an endpoint. This will create a snapshot of the pipeline in the current state and link an endpoint to it.

1. Make sure you are happy with all node names, especially inputs ans outputs. See [Naming nodes](#naming-nodes).
1. Click "Deploy" at the top of the page.
1. Name this deployment so that you can identify it in the history in the future.
1. Choose an endpoint to deploy your pipeline to or (soon) create a new endpoint from the same page.

After the deployment is successful, you can go to the endpoints page and see the deployment you've just made. For more, read Endpoints  (TODO).
